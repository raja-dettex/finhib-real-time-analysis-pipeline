version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.1.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - confluent

  kafka:
    image: confluentinc/cp-kafka:6.1.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081
    healthcheck:
      test: ["CMD", "bash", "-c", "nc -z localhost 9092"]
    networks:
      - confluent

  kafka-init:
    image: confluentinc/cp-kafka:6.1.0
    container_name: kafka-init
    depends_on:
      kafka:
        condition: service_healthy
      zookeeper:
        condition: service_healthy
    networks:
      - confluent
    entrypoint: /bin/bash
    command: -c "sleep 20 && kafka-topics --create --topic live-stock --bootstrap-server kafka:29092 --replication-factor 1 --partitions 1"
    healthcheck:
      test: ["CMD", "bash", "-c", "kafka-topics --list --bootstrap-server kafka:29092"]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s

  schema-registry:
    image: confluentinc/cp-schema-registry:6.1.0
    container_name: schema-registry
    depends_on:
      zookeeper:
        condition: service_healthy
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_started
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    networks:
      - confluent
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/"]
      interval: 30s
      timeout: 10s
      retries: 5

  schema-init:
    build:
      context: ./schema_registry/
      dockerfile: Dockerfile
    container_name: schema-init
    depends_on:
      zookeeper:
        condition: service_healthy
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      kafka-init:
        condition: service_started
    networks:
      - confluent
    entrypoint: /bin/bash
    command: -c "sleep 20 && python /temp/script/create-schema.py schema-registry:8081"

  producer:
    build:
      context: ./producer/
      dockerfile: Dockerfile
    container_name: producer
    entrypoint: /bin/bash
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      schema-init:
        condition: service_started
    networks:
      - confluent
    command: -c "sleep 20 && python /app/src/finhub_api.py"

  rest-proxy:
    image: confluentinc/cp-kafka-rest:6.1.0
    container_name: rest-proxy
    depends_on:
      zookeeper:
        condition: service_healthy
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "8082:8082"
    networks:
      - confluent
    environment:
      KAFKA_REST_HOST_NAME: rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'kafka:29092'
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'

  spark-master:
    image: bitnami/spark:latest
    container_name: spark_master
    command: /entrypoint.sh bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      - confluent
    volumes:
      - ./processor/src/main.py:/work/main.py
      - ./processor/entrypoint.sh:/entrypoint.sh

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark_worker
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077
    networks:
      - confluent

  cassandra:
    image: cassandra:latest
    container_name: cassandra
    ports:
      - "9042:9042"
      - "7000:7000"
      - "7001:7001"
      - "7199:7199"
      - "9160:9160"
    environment:
      - MAX_HEAP_SIZE=256M
      - HEAP_NEWSIZE=128M
    volumes:
      - cassandra_data:/var/lib/cassandra
    healthcheck:
      test: ["CMD", "cqlsh", "--execute", "DESCRIBE KEYSPACES;"]
      interval: 10s
      timeout: 10s
      retries: 10
    command: ["cassandra", "-f"]

volumes:
  cassandra_data:
    driver: local

networks:
  confluent:
